{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# general challenges: \n",
    "- a) trouble with Google Cloud: some team members who hadn't used it for assignment 1 had trouble setting everything up. Although it says \"on your local computer\" in the GCP \"manual\", it didn't work in the cmd.exe, but we had to use teh cloud shell. Furthermore when we had set everything up and chosen our GPU, it didn't connect via localhost:8081 and we had to connect it with some cmd prompts. One team member lost the 50 dollar entirely (due to own stupidness) because he'd forgotten to shut his instance down. \n",
    " -b) apparently different versions of Anaconda,sklearn etc installed on our laptops: Module SimpleImputer didn't exist on everybody's laptop...\n",
    " -c) big dataset with few explanations -> exercise A was the by far most difficult exercise for us and time-consuming. \n",
    " -d) imperfect dataset (altough we know that this'll be the case in real cases): e.g. datatype float where the variable should have been an Integer. \n",
    " -e) hidden information in the dataset: when half of the values was missing, this didn't necessarily mean that the data wasn't there. When the rest of the values was \"Y\", a missing value could mean \"No\". \n",
    " -f) well, the division into A-C isn't 100% clear since for us, it seemed like one big task. As such, it was also difficult to split the task into 2 persons. Doing B and C requires a good knowledge of the sfPermits dataframe. \n",
    " - g) many \"permitted\", few \"not permitted\" (current status)\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# --- and how we solved them: \n",
    "- a) run everything locally :D (we thought that imputing values would be time-consuming, but this was not the case fortunately)\n",
    "- b) didn't use the module. Instead, used the following code (part A): \n",
    "for column in columns_for_mode:\n",
    "    imputed_permits[column].fillna(imputed_permits[column].mode()[0], inplace=True)\n",
    "    \n",
    "    \n",
    "imputed_permits[column].fillna( imputed_permits[column].mean(skipna=True) , inplace=True)\n",
    "\n",
    "and in the one version whenever we printed a table the shape of it was printed, too. On my laptop I printed the shape manually when useful, as I didn't have this auto-shape-printing...\n",
    "\n",
    "- c)lucky for every column we dropped. Strategy: look for columns where share(NaN) > 50%\n",
    "    then: think of context -> kill IDs etc\n",
    "    cf. ex A\n",
    "-  d) look at the dataset more closely and use common sense... Drop definitions/explanations (String)\n",
    "- e) due to time constraints, we didn't implement it and dropped the column. \n",
    "- f) notebook A became very large. In order to leave this code untouched, we saved our work in a CSV file which was very easy to open in the notebook for B+C. This also makes orientation in the code easier. \n",
    " -g)  opening 2 categories as 1st step. However, division into the 2 categories was 9:1. So, correlations almost all look very simlar. We tried our best thinking of related things and trying it out by plotting it. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
